
<html>
<head>
<title>Data IAP Day 1</title>
<link rel="stylesheet" type="text/css" href="clearness.css"/>
</head>
<body>
<h1>Day 4: Text Processing</h1>
<h2>Overview</h2>
<p>Today we will discuss text processing.  Our exercises will be grounded in an email dataset (either yours or Kenneth Lay's).  After today, you will be able to compute the most important terms in a particular email, find emails similar to the one you are reading, and find people that tend to send you similar emails.</p>
<p>As opposed to the data we've seen so far, we will need to clean the data before we can extract meaningful results.</p>
<p>The techniques will include</p>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Tfidf">Tf-Idf</a></li>
<li><a href="http://en.wikipedia.org/wiki/Regular_expression">Regular Expressions</a> and data cleaning</li>
<li><a href="http://en.wikipedia.org/wiki/N-gram">N-gram</a></li>
<li><a href="http://en.wikipedia.org/wiki/Cosine_similarity">Cosine Similarity</a></li>
</ul>
<h2>Setting Up</h2>
<h3>Datasets</h3>
<h4>1. Your Own Email</h4>
<p>We have provided a <a href="">script</a> that you can use to download your email.  However before you can run it, you will need to install the following python modules:</p>
<ul>
<li><a href="">dateutil</a></li>
<li><a href="">pyparsing</a></li>
</ul>
<p>You can now run the script using the following command:</p>
<pre><code>python download_emails.py [IMAP ADDRESS]
</code></pre>
<p>You can pass the optional imap address parameter, otherwise it will default to gmail's imap address.  The script will then ask you to input your email and password, then create the folder <code>email_root/</code> and download your email folders into that directory.</p>
<h4>2. Kenneth Lay's Emails</h4>
<p>Alternatively, if you would rather not use your own emails, or could not get the script working, you can download and unzip Kenneth Lay's  (Pre-bankruptcy Enron CEO) emails that were made public after the accounting fraud scandal.</p>
<p><a href="">Download the emails</a></p>
<h3>Reading Email Files</h3>
<p>You will need some code to parse and read the emails.  <code>email_root</code> contains a bunch of folders like <code>_sent</code> (sent folder), <code>inbox</code>, and other folders depending how Kenneth or you organized your emails.  Each folder contains a list of files.  Each file corresponds to a single email.</p>
<p>We have written a module that makes it easier to manage the emails.  To use it add the following import</p>
<pre><code>import sys
sys.path.append('PATHTODATAIAP/resources/util/')
import email_util
</code></pre>
<p>The module contains two classes, <code>Email</code> and <code>EmailWalker</code>.  The first class reads and parses an email into an easy to use object and the second class iterates through and creates an <code>Email</code> object for each email file in the directory.</p>
<p><strong><code>Email</code></strong></p>
<ul>
<li><code>__init__(self, file_path)</code>: pass in the path to an email file.</li>
<li>Class Members<ul>
<li><code>path</code>: full path to the email file in the file system</li>
<li><code>folder</code>: name of the folder the email is in (e.g., inbox, _sent)</li>
<li><code>frame</code>: name of the email file in the file system</li>
<li><code>sender</code>: the email address of the sender</li>
<li><code>recipients</code> a list containing all emails in the <code>to</code>, <code>cc</code>, and <code>bcc</code> fields</li>
<li><code>to</code>: list of emails in <code>to</code> field</li>
<li><code>cc</code>: list of emails in <code>cc</code> field</li>
<li><code>bcc</code>: list of emails in <code>bcc</code> field</li>
<li><code>subject</code>: subject line</li>
<li><code>date</code>: <code>datetime</code> object of when the email was sent</li>
<li><code>text</code>: the text content in the email</li>
</ul>
</li>
</ul>
<p><strong><code>EmailWalker</code></strong></p>
<ul>
<li><code>__init__(self, email_root)</code>: pass in the root directory containing the email folders (e.g., inbox, _sent)</li>
<li><code>__iter__(self)</code>: returns an iterator that returns email objects</li>
</ul>
<p>The <code>__iter__</code> method means that <code>EmailWalker</code> is an iterator, and can be conveniently used in a loop:</p>
<pre><code>walker = EmailWalker('./email_root')
for email_object in walker:
    print email_object.subject
</code></pre>
<h2>Folder Summaries</h2>
<p>In this section, we will automatically extract key terms that describe the emails in a folder.  This is useful for two purposes.  First, it can be a crude summary of the emails in each folder.  Second, it is used to search for and retrieve emails.  For example, if a term (e.g., "lawsuit") is representative of an email, then we would like to retrieve that email when we search for "lawsuit".<br />
</p>
<p>Today, we will focus on the first purpose.</p>
<h3>Term Frequency (TF)</h3>
<p>One way to do this is to count the number of times each term occurs in all of the emails in a folder.  The term that comes up the most must be best represent the folder!</p>
<pre><code>import os, sys, math
sys.path.append('./util')
from email_util import *
from collections import Counter, defaultdict

folder_tf = defaultdict(Counter)

for e in EmailWalker(sys.argv[1]):
    terms_in_email = e.text.split() # split the email text using whitespaces
    folder_tf[e.folder].update(terms_in_email)

for folder, counter in folder_tf.items():
    print folder
    for pair in sorted(counter.items(), key=lambda (k,v): v, reverse=True)[:20]:
        print '\t', pair
</code></pre>
<p>But if we take a look at the output, they are non-descriptive terms that are simply used often.  There are also random characters like <code>&gt;</code>, which are clearly not words, but happen to pop-up often.</p>
<pre><code>sent
    ('the', 2529)
    ('to', 2041)
    ('and', 1357)
    ('of', 1203)
    ('in', 905)
    ('a', 883)
    ('&gt;', 834)
</code></pre>
<p>This suggests that we need a better approach than term frequency, and that we need to clean the email text a bit.  We will walk you through how to do then in that order.</p>
<h3>Term Frequency - Inverse Document Frequency (<a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a>)</h3>
<p>Instead of the most popular terms, what we want popular items above background noise.  For example, "the" would be considered background noise because it is found multiple times in nearly every single email, so it is not very descriptive.  Similarly, "enron" is probably not very descriptive because we would expect most emails to mention the term.<br />
</p>
<p>TF-IDF is a widely used metric that captures this idea by combining two intuitions.  The first intuition is Term Frequency, and the second is Inverse Document Frequency:</p>
<ol>
<li>we want to increase a term's weight if it occurs often in a folder</li>
<li>we want to decrease a term's weight if it's also found in the other folders.</li>
</ol>
<p>A term's IDF value is formally computed as </p>
<pre><code>log( total # documents / # documents that contain term )
</code></pre>
<p>In our case, the numerator is the total number of emails and the denominator is the number of emails containing the term.  Finally, the TF-IDF is simply a multiple of the two values:</p>
<pre><code>TF * IDF
</code></pre>
<p>The following code will construct a dictionary that maps a term to its IDF value:</p>
<pre><code>allterms = Counter()
nemails = 0
for e in EmailWalker(sys.argv[1]):
    terms_in_email = e.text.split() # split the email text using whitespaces
    unique_terms_in_email = set(terms_in_email)
    allterms.update(unique_terms_in_email)
    nemails += 1

idfs = {}
for term, count in allterms.iteritems():
    idfs[term] = math.log( nemails / (1 + allterms[term]) )

tfidfs = {} # key is folder name
for folder, tfs in folder.iteritems:
    # write code to calculate tf-idfs yourself!
    pass
</code></pre>
<p>If we combine <code>idfs</code> with each folder's <code>tf</code> value, we would compute the <code>tf-idf</code>.  If we print the top values for each folder, we would see something like:</p>
<pre><code>inbox
    ('&gt;', 10022.526185338656)
    ('i', 3117.082870978074)
    ('=', 2287.3107850070046)
    ('&lt;td', 1898.8767820921892)
    ('my', 1831.540344350006)
    ('our', 1706.1448843015744)
    ('will', 1703.0626226357856)
    ('it', 1691.8629245488892)
    ('have', 1689.8928262051465)
    ('was', 1660.9399256319914)
</code></pre>
<p>As we can see, there is a lot of noise, and non-word characters pop up a lot.  We will deal with this next.</p>
<p>In addition, there are a number of extensions</p>
<h3>Regular Expressions and Data Cleaning</h3>
<p>The email dataset is a simple dump, and each file contains the email headers, attachments, and the actual message -- all of it is ascii-encoded.  In order to see sensible terms, we need to clean the data a bit.  This process varies depending on what your application is.  In our case, we decided that we want</p>
<ol>
<li>We don't care about casing.  We want "enron" and "Enron" to be the same term.</li>
<li>We don't care about really short words.  We want words with 4 or more characters. </li>
<li>We don't care about <a href="http://en.wikipedia.org/wiki/Stop_words">stop words</a>.  We pre-decided that words like "the" and "and" should be ignored.</li>
<li>Reasonable words.  These should only contain a-z characters, hyphens, and apostrophes.  It should also start and end with an a-z character.</li>
</ol>
<p>Let's tackle each of these requirements one by one!</p>
<h4>1-2. Casing and Short Words</h4>
<p>We can deal with these by lower casing all of the terms and filtering out the short terms.</p>
<pre><code>terms = e.text.lower().split()
terms = filter(lambda term: len(term) &lt;= 3, terms)
</code></pre>
<h4>3. Stop Words</h4>
<p>The <code>email_util</code> module defines a variable <code>STOPWORDS</code> that contains a list of common english stop words in lower case.  We can filter out terms that are found in in this list.</p>
<pre><code>from email_util import STOPWORDS
terms = filter(lambda term: term in STOPWORDS, terms)
</code></pre>
<h4>4. Reasonable Words (Regular Expressions)</h4>
<p>The last requirement is more difficult to enforce.  One way is to iterate through the characters in every term, and make sure they are valid:</p>
<pre><code>arr = e.text.split()
terms = []
for term in arr:
    valid = True
    for idx, c in  enumerate(term.lower()):
        if (idx == 0 or idx == len(term)-1):
            if (c &lt; 'a' or c &gt; 'z'):
                valid = False
                break
        elif (c != "'" and c != "-" and (c &lt; 'a' or c &gt; 'z')):
            valid = False
            break
    if valid:
        terms.append(term)
</code></pre>
<p>This is a pain in the butt to write, and is hard to understand and change.  All we are doing is making sure each term adheres to a pattern.  Regular Expressions (regex) is a very convenient language for finding and extracting patterns in text.  We don't have time for a complete tutorial, but we will talk about the basics.</p>
<p>Regex lets you specify:</p>
<ul>
<li>Classes of characters.  You may only care about upper case characters, or only digits and hyphens.<br />
</li>
<li>Repetition.  You can specify how many times a character or pattern should be repeated.</li>
<li>Location of the pattern.  You can specify that the pattern should be at the beginning of the term, or the end.</li>
</ul>
<p>It's easiest to show examples, so here's code that defines a pattern of strings that start with either <code>e</code> or <code>E</code>, followed the characters <code>nron</code>.  <code>re.search</code> checks if the pattern is found in <code>term</code> and returns <code>None</code> if the pattern was not found.</p>
<pre><code>import re
term = "enronbankrupt"
pattern = "[eE]nron+"
if re.search(pattern, term):
    print "found!"
</code></pre>
<p>The most basic pattern is a list of characters.  <code>pattern = "enron"</code> looks for the exact string <code>"enron"</code> (lower case).  But what if we want to match <code>"Enron"</code> and <code>"enron"</code>?  That's where character classes come in!</p>
<p>Brackets <code>[]</code> are used to define a character class.  That means any character in the class would be matched.  You simply list the characters that are in the class.  For example <code>[eE]</code> matches both <code>e</code> and <code>E</code>.  Thus <code>[eE]nron</code> would match both <code>"Enron"</code> and <code>"enron"</code>.  <code>[0123456789\-]</code> means that all digits and hyphens should be matched.  We need to escape <code>-</code> within <code>[]</code> because it is a special character.</p>
<p>It's tedious to list individual characters, so <code>-</code> can be used to specify a range of characters.  <code>[a-z]</code> is all characters between lower case <code>a</code> and <code>z</code>.  <code>[A-Z]</code> are all upper case characters.  <code>[a-zA-Z]</code> are all upper or lower case characters.  There are other shortcuts for common classes.  For example, <code>\w</code> is shorthand for <code>[a-zA-Z0-9]</code></p>
<p><code>[a-z]</code> only matches a single character.  We can add a special character at the end of the class to specify how many times it should be repeated:</p>
<ul>
<li><code>?</code>: 0 or 1 times.  For optional characters</li>
<li><code>*</code>: 0 or more times.</li>
<li><code>+</code>: 1 or more times</li>
<li><code>{n}</code>: exactly <code>n</code> times</li>
<li><code>{n,m}</code>: between <code>n</code> and <code>m</code> times (inclusive).</li>
</ul>
<p>For example, <code>[0-9]{3}-[0-9]{3}-[0-9]{4}</code> matches phone numbers that contain area codes.  Note that we didn't escape the <code>-</code> because it specifies a range within <code>[]</code> and is not interpreted as a range outside the <code>[]</code>.  This pattern fails if the user inputs <code>(510)-232-2323</code> because it doesn't recognize the <code>()</code>.  Can you modify the pattern to optionally allow <code>()</code>?</p>
<p>Finally, <code>^</code> and <code>$</code> are special characters for the beginning and the end of the text, respectively.  For example <code>^enron</code> means that <code>"enron"</code> must be at the beginning of the string.  <code>enron$</code> means that the <code>"enron"</code> should be at the end.  <code>^enron$</code> means the term should be exactly <code>"enron"</code>.</p>
<p>Great!  You should know enough to create a pattern to find "reasonable words", and use it to re-compute the <code>tfidfs</code> dictionary!</p>
<h2><a href="http://en.wikipedia.org/wiki/Cosine_similarity">Cosine Similarity</a></h2>
<p>It would be helpful to find email senders that send similar emails to Kenneth Lay.  That way, if we are reading an interesting email about Enron's bankruptcy, we can find other people that have sent similar emails.  <a href="http://en.wikipedia.org/wiki/Cosine_similarity">Cosine similarity</a> is a common tool to achieve this.</p>
<p>The main idea is that emails that share terms with high tf-idf values are probably similar.  Also, they are more similar if they share more terms.<br />
</p>
<p>Let's say we have a total of 1000 terms across all of the email senders.  Every email sender has a tf-idf score for each of the 1000 terms.  We could model all of the scores as a 1000-dimensional vector, where each dimension corresponds to a term, and the distance along the dimension is the term's tf-idf value.  The cosine of the two email senders' vectors measures the similarity between them.  -1 means they are completely opposite, and 1 means they are identical.  0 means the senders are independent from each other (the vectors are orthogonal).<br />
</p>
<p>Here is how we would calculate the cosines similarity of two <em>folders</em>, using the <code>tfidfs</code> dictionary you computed in the previous section.</p>
<pre><code>sec_scores = tfidfs['sec_panel']
</code></pre>
<p>We model every email sender's (term, tfidf score) pairs as a vector</p>
<p>We warn you that running this can take a while.  There are 2000 emails </p>
<p>If we are reading an email, we would like to find other similar emails.  Cosine similarity is a common tool </p>
<h2>N-grams</h2>
<p>Finally, only one word per term.  Not really clear.  "expensive", even though one could be part of the phrase "not expensive" whereas the other is "very expensive".  One popular way to add more context is to simply use more than one word per term (notice that we've used the word "term" instead of word for this reason).</p>
<h2>Done!</h2>
<h2>Notes</h2>
<p>Email dataset</p>
<ul>
<li>Email script to download gmail email messages without attachments</li>
<li>Download the folders</li>
<li>Extract the data into files.<br />
</li>
<li>1 file per email</li>
</ul>
<p>TF-IDF</p>
<p>Enron dataset
<em> Kenneth Lay (lay-k)
</em> Jeffery Skilling
* <code>lay-k/ skilling-j/ whalley-g/ pereira-s/</code></p>
</body>
</html>

